{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a397f8-f97f-42c8-b9b6-eeeb99b7b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import openai\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(parent_dir)\n",
    "sys.path.append('/home/or/dev/latin_music_statistics/crawler/data_dir')\n",
    "sys.path.append(os.path.abspath(os.path.join(parent_dir, 'statistics_dir')))\n",
    "\n",
    "from config import *\n",
    "\n",
    "client = OpenAI(api_key = OPENAI_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b1ebf22-934a-4c6b-ae8e-c4891cd80d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "723af417-bd9f-426f-b8a2-ab4372794a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt: 1 to exrect the python list form the text\n",
      "Processed batch 0 to 100\n",
      "Attempt: 1 to exrect the python list form the text\n",
      "Processed batch 100 to 200\n",
      "Processed batch 200 to 300\n",
      "Processed batch 300 to 400\n",
      "Processed batch 400 to 500\n",
      "Attempt: 1 to exrect the python list form the text\n",
      "Processed batch 500 to 600\n",
      "Attempt: 1 to exrect the python list form the text\n",
      "Processed batch 600 to 700\n",
      "Attempt: 1 to exrect the python list form the text\n",
      "Processed batch 700 to 800\n",
      "Processed batch 800 to 900\n",
      "Initial number of clusters: 585\n",
      "Processed batch 0 to 100\n",
      "Processed batch 100 to 200\n",
      "Attempt: 1 to exrect the python list form the text\n",
      "Processed batch 200 to 300\n",
      "Attempt: 1 to exrect the python list form the text\n",
      "Processed batch 300 to 400\n",
      "Processed batch 400 to 500\n",
      "Processed batch 500 to 600\n",
      "The number of clusters: 247\n",
      "Processed batch 0 to 100\n",
      "Attempt: 1 to exrect the python list form the text\n",
      "Processed batch 100 to 200\n",
      "Processed batch 200 to 300\n",
      "The number of clusters: 152\n",
      "Processed batch 0 to 100\n",
      "Processed batch 100 to 200\n",
      "The number of clusters: 37\n",
      "Processed batch 0 to 100\n",
      "The number of clusters: 19\n",
      "The final number of clusters: 19\n"
     ]
    }
   ],
   "source": [
    "MAX_THEME_CLUSTER_SIZE = 20\n",
    "THEME_BATCH_SIZE = 100\n",
    "\n",
    "SONGS_CLUSTERING_PROMPT = \"Cluster the following song themes into brief general themes:\\n\\n{batch_text}\\n\\nPlease provide the clusters in a clean Python list format without any extra text or formatting. The list should be written exactly as a Python list, like this: [\\\"...\\\",\\\"...\\\",\\\"...\\\"].\\n\\nEnsure there are no additional markers or explanationsâ€”just the list.\"\n",
    "CLUSTERS_PROMPT = f'These song theme clusters\\nGroup these song themes into broader, more concise categories, combining similar themes and removing redundancy.\\nCombine them into less than {MAX_THEME_CLUSTER_SIZE} ' + 'categories\\nThe output should look like: [\"...\",\"...\",\"...\"].\\n{batch_text}\\nEnsure all clusters are combined into a single, flat Python list with no nested lists.'\n",
    "\n",
    "OPENAI_MODEL = \"gpt-4o\"\n",
    "import ast\n",
    "\n",
    "def chatgpt_request_list_extraction(msgs, model_name):\n",
    "    response = client.chat.completions.create(model=model_name, messages=msgs)\n",
    "    return response\n",
    "    \n",
    "def extract_valid_list(response_text, model_name, max_attempts=5):\n",
    "    attempt = 0\n",
    "    chatgpt_messages = []\n",
    "\n",
    "    while attempt < max_attempts:\n",
    "\n",
    "        try:\n",
    "            extracted_list = ast.literal_eval(response_text)\n",
    "            if isinstance(extracted_list, list):\n",
    "                return extracted_list\n",
    "                \n",
    "        except (SyntaxError, ValueError):\n",
    "            print(f\"Attempt: {attempt + 1} to exrect the python list form the text\")\n",
    "            attempt += 1\n",
    "            prev_content = f\"The previous {attempt + 1} attempt to extract of the valid python list from the text failed.\\n Wxtract valid Python list with no additional text, formatting, or code. The output should look exactly like this: [\\\"...\\\"].\\nProvide only the list, nothing else.\"\n",
    "            chatgpt_messages = prepare_chatgpt_msg(response_text, prev_content, chatgpt_messages)\n",
    "            extraction_response = chatgpt_request_list_extraction(chatgpt_messages, model_name)\n",
    "            response_text = extraction_response.choices[0].message.content\n",
    "    \n",
    "    raise ValueError(\"Failed to extract a valid Python list after multiple attempts.\")\n",
    "\n",
    "def prepare_chatgpt_msg(curr_text, prev_text, chatgpt_messages):\n",
    "    chatgpt_messages.append({\"role\": \"user\", \"content\": prev_text})\n",
    "    chatgpt_messages.append({\"role\": \"user\", \"content\": curr_text})\n",
    "    return chatgpt_messages\n",
    "\n",
    "def python_list_to_batch_text_promt(base_prompt, a_list, prev_text, batch_size, row_i, max_len):\n",
    "    batch_text = \"\\n\".join(a_list[row_i: min(row_i + batch_size, max_len)])\n",
    "    prompt = base_prompt.format(batch_text=batch_text)\n",
    "    chatgpt_msg_request =  prepare_chatgpt_msg(prompt, prev_text, [])\n",
    "    return chatgpt_msg_request\n",
    "\n",
    "def check_max_clusters(failed_extraction, max_theme_clusters_size ,len_clusters_before):\n",
    "    return (not failed_extraction) and (max_theme_clusters_size < len_clusters_before)\n",
    "\n",
    "\n",
    "def batch_process_themes(list_to_cluster, prev_text, base_prompt, theme_batch_size, model):\n",
    "    row_i = 0\n",
    "    total_clusters = []\n",
    "    total_len =len(list_to_cluster)\n",
    "    \n",
    "    while row_i <= total_len:\n",
    "        chatgpt_msg_request = python_list_to_batch_text_promt(base_prompt, list_to_cluster, prev_text, theme_batch_size, row_i, total_len)\n",
    "        response = chatgpt_request_list_extraction(chatgpt_msg_request, model)\n",
    "        curr_batch_size = theme_batch_size\n",
    "\n",
    "        # Check if the response is not finished properly\n",
    "        while response.choices[0].finish_reason != \"stop\":\n",
    "            print(f\"Batch {row_i} to {row_i + curr_batch_size} didn't complete. Reducing batch size.\")\n",
    "            curr_batch_size = max(1, curr_batch_size // 2)  # Reduce batch size by half, but not below 1\n",
    "            chatgpt_msg_request = python_list_to_batch_text_promt(base_prompt, batch_list, prev_text, theme_batch_size, row_i, total_len)\n",
    "            response = chatgpt_request_list_extraction(chatgpt_msg_request, model)\n",
    "\n",
    "        response_text = response.choices[0].message.content\n",
    "\n",
    "        try:\n",
    "            response_as_list = extract_valid_list(response_text, OPENAI_MODEL, max_attempts=5)\n",
    "        except ValueError as e:\n",
    "            print(f\"Failed to extract valid list from CHATGPT prompt\")\n",
    "            return(total_clusters) ,1\n",
    "            \n",
    "        total_clusters.extend(response_as_list)\n",
    "        print(f\"Processed batch {row_i} to {row_i + THEME_BATCH_SIZE}\")\n",
    "\n",
    "        row_i += curr_batch_size\n",
    "        \n",
    "    return total_clusters, 0 \n",
    "\n",
    "prev_text = ''\n",
    "song_list = df['theme'].tolist()\n",
    "\n",
    "clustered_themes, failed_extraction = batch_process_themes(song_list, prev_text, SONGS_CLUSTERING_PROMPT, THEME_BATCH_SIZE, OPENAI_MODEL)\n",
    "\n",
    "len_clusters_before = len(clustered_themes)\n",
    "continue_clustring = check_max_clusters(failed_extraction, MAX_THEME_CLUSTER_SIZE, len_clusters_before)\n",
    "\n",
    "\n",
    "decreasing_clusters_attempt = 1\n",
    "print(f\"Initial number of clusters: {len_clusters_before}\")\n",
    "\n",
    "while continue_clustring:\n",
    "    curr_clustered_themes, failed_extraction = batch_process_themes(clustered_themes, prev_text, CLUSTERS_PROMPT, THEME_BATCH_SIZE, OPENAI_MODEL)\n",
    "    prev_clusters_text = '\\n'.join(clustered_themes)\n",
    "    curr_clusters_text = '\\n'.join(curr_clustered_themes)\n",
    "    len_clusters_after = len(curr_clustered_themes)\n",
    "    \n",
    "    print(f\"The number of clusters: {len_clusters_after}\")\n",
    "\n",
    "    if len_clusters_before == len_clusters_after:\n",
    "        decreasing_clusters_attempt += 1\n",
    "        prev_text =  f'This is the {decreasing_clusters_attempt} attempt for clustering. ' +\\\n",
    "                     f'There are {len_clusters_after} many clusters - which are too many. ' +\\\n",
    "                     f'The previous clustered topic attempt was:\\n{prev_clusters_text}\\n\\n\\nTry again with those topics:\\n' + curr_clusters_text\n",
    "    else:\n",
    "        decreasing_clusters_attempt = 1\n",
    "        prev_text = ''\n",
    "\n",
    "    continue_clustring = check_max_clusters(failed_extraction, MAX_THEME_CLUSTER_SIZE, len_clusters_after)\n",
    "\n",
    "    clustered_themes = curr_clustered_themes\n",
    "    len_clusters_before = len_clusters_after\n",
    "\n",
    "print(f\"The final number of clusters: {len(clustered_themes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838259f-1c30-4892-9c60-9767e8b40c81",
   "metadata": {},
   "source": [
    "# create specific song themes embeddings and match the closest general theme embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef1c7ee4-e624-437b-afa9-ee7bc59b9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_themes_df = pd.read_csv(GENERAL_SONGS_THEMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a2d15ae-bbb8-4a6a-90ae-ec61cdece85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/or/dev/latin_music_statistics/lms/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24644bf8d6e94801a473d323b1d708fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 10.84 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36f44ed8cf4466abe439fc67d5cf701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.09 minutes\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.autonotebook import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize the Hugging Face model\n",
    "MODEL_NAME = 'sentence-transformers/all-roberta-large-v1'\n",
    "\n",
    "emb_model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "def create_embeddings(df, model, src_col, tgt_col):\n",
    "    df[tgt_col] = None\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    # Use tqdm to add a progress bar to the loop\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "        response_emb = model.encode(row[src_col])    \n",
    "        df.at[index, tgt_col] = response_emb\n",
    "    \n",
    "    # Track the end time\n",
    "    toc = time.time()\n",
    "    \n",
    "    # Print the time taken in minutes\n",
    "    print(f\"Time taken: {(toc - tic) / 60:.2f} minutes\")\n",
    "\n",
    "create_embeddings(df, emb_model, 'theme', 'theme_emb')\n",
    "create_embeddings(general_themes_df,  emb_model, 'general_theme', 'general_theme_emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860ed45f-d85f-4700-8b57-58925e80ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "def find_closest_embeddings(df, general_themes_df):\n",
    "    # Assuming embeddings are stored as lists or numpy arrays in the DataFrame columns\n",
    "    df['closest_general_theme'] = None\n",
    "\n",
    "    for idx, theme_emb in df.iterrows():\n",
    "        current_embedding = theme_emb['theme_emb']\n",
    "        \n",
    "        # Initialize variables to find the closest match\n",
    "        min_dist = float('inf')\n",
    "        closest_theme = None\n",
    "        \n",
    "        # Iterate over general_themes_df to find the closest match\n",
    "        for _, general_row in general_themes_df.iterrows():\n",
    "            general_embedding = general_row['general_theme_emb']\n",
    "            general_theme = general_row['general_theme']\n",
    "            \n",
    "            # Compute cosine similarity (distance) between the current embedding and the general embeddings\n",
    "            dist = cosine(current_embedding, general_embedding)\n",
    "\n",
    "            # Update closest match if a smaller distance is found\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                closest_theme = general_theme\n",
    "        \n",
    "        # Store the closest theme in the df\n",
    "        df.at[idx, 'general_theme'] = closest_theme\n",
    "\n",
    "    return df\n",
    "\n",
    "result_df = find_closest_embeddings(df, general_themes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9394b0b-48c6-49b7-a876-21af3506f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed_columns = [col for col in result_df.columns if 'Unnamed' in col]\n",
    "result_df.drop(unnamed_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1beca82d-93f6-4da4-af0f-2058d21b76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538561c-8ea7-49c0-99c6-d84607f88d92",
   "metadata": {},
   "source": [
    "# Match the general categry to each theme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "35c57ef4-707f-4d3d-804b-82e7382624fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "# MATCH_PROMPT = 'To which of the following general categories:\\n{general_themes}\\nThis theme fits best:\\n{cur_theme}\\nwrite only the catagory name:'\n",
    "# WRONG_GENERAL_THEME =  'This is the {attempt_num} attempt.\\n' +\\\n",
    "#              'You failed to extract the best fitting general theme out of the theme list.\\n' +\\\n",
    "#              'You wrote name {name} which is not in the list.' + curr_clusters_text\n",
    "\n",
    "# df_general_themes = pd.read_csv(GENERAL_SONGS_THEMS)\n",
    "\n",
    "# unnamed_columns = [col for col in df.columns if 'Unnamed' in col]\n",
    "# df.drop(unnamed_columns, axis=1, inplace=True)\n",
    "\n",
    "# def prepare_chatgpt_msg(curr_text, prev_text, chatgpt_messages):\n",
    "#     chatgpt_messages.append({\"role\": \"user\", \"content\": prev_text})\n",
    "#     chatgpt_messages.append({\"role\": \"user\", \"content\": curr_text})\n",
    "#     return chatgpt_messages\n",
    "\n",
    "# def match_generic_theme(theme_txt, general_themes):\n",
    "#     attempt_num = 1\n",
    "#     curr_text = MATCH_PROMPT.format(general_themes=','.join(general_themes), cur_theme=theme_txt)\n",
    "#     chatgpt_msg_request = prepare_chatgpt_msg(curr_text, '', [])\n",
    "#     response = chatgpt_request_list_extraction(chatgpt_msg_request, OPENAI_MODEL)\n",
    "\n",
    "#     response_txt = response.choices[0].message.content\n",
    "#     while response_txt not in general_themes:\n",
    "#         attempt_num += 1\n",
    "#         prev_text = WRONG_GENERAL_THEME.format(attempt_num=attempt_num, name=response_txt)\n",
    "\n",
    "#         chatgpt_msg_request = prepare_chatgpt_msg(curr_text, prev_text, [])\n",
    "#         response = chatgpt_request_list_extraction(chatgpt_msg_request, OPENAI_MODEL)\n",
    "#         response_txt = response.choices[0].message.content\n",
    "\n",
    "#         print(f'{attempt_num}:\\n\\nprev_text {prev_text}')\n",
    "#         print('-'*100)\n",
    "#     return response  \n",
    "\n",
    "# df['general_theme'] = \"\"\n",
    "\n",
    "# # Use tqdm to add a progress bar to the loop\n",
    "# for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "#     print(index)\n",
    "#     result = match_generic_theme(row['theme'], general_themes)\n",
    "    \n",
    "#     # Add or set the result to the 'general_theme' column\n",
    "#     df.at[index, 'general_theme'] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d7cca-a928-4b8c-a96b-45b88522054c",
   "metadata": {},
   "source": [
    "df['general_theme'] = ''\n",
    "general_themes = df_general_themes['general_theme'].tolist()\n",
    "df['general_theme'] = df['theme'].apply(lambda txt: match_generic_theme(txt, general_themes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd83f26b-e993-43c6-bcf2-b4af0b81902b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692940d7-cebe-492e-8502-e486cd85090f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b865766-2e33-4c10-923f-a445b14a3bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2997600-6e02-4e80-8a40-408c8750e6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda803c0-1b5a-4210-87c0-a3d0e72c4228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa064a-9a6d-4f89-a38c-a3f345fe9415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15cfb23-ca83-4872-a104-10b311b06518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d79b8b-1bb1-4d9e-83c7-3272d0a0fd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6083fc-7462-42ae-a867-0b0c23d9f747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
